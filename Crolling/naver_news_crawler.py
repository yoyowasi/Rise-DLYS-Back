import asyncio
import datetime
import os
import re
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin, urlparse

import aiohttp
from bs4 import BeautifulSoup
import mysql.connector

from dotenv import load_dotenv
load_dotenv()

# --- í™˜ê²½ ì„¤ì • ---
DB_CONFIG = {
    "host": os.environ.get("DB_HOST", "dlys-database.c41q6i80on7q.us-east-1.rds.amazonaws.com"),
    "user": os.environ.get("DB_USER", "daelim"),
    "password": os.environ.get("DB_PASSWORD", "leeyoosong"),
    "database": os.environ.get("DB_DATABASE", "dlys"),
    "port": int(os.environ.get("DB_PORT", "3306")),
}

# --- ê³µí†µ ìƒìˆ˜ ---
USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/127.0.0.0 Safari/537.36"
)
REQUEST_TIMEOUT = aiohttp.ClientTimeout(total=15)
CONCURRENCY_LIMIT = 10
FETCH_RETRIES = 3
RETRY_BASE_DELAY = 0.7  # seconds

# ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ URL ì •ê·œì‹ (ì˜ˆ: https://n.news.naver.com/article/001/0012345678)
ARTICLE_URL_RE = re.compile(r"^https?://n\.news\.naver\.com/article/\d+/\d+")

# --- HTTP ìš”ì²­ ---
async def fetch(session: aiohttp.ClientSession, url: str) -> Optional[str]:
    headers = {
        "User-Agent": USER_AGENT,
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Referer": "https://news.naver.com/",
    }
    for attempt in range(1, FETCH_RETRIES + 1):
        try:
            async with session.get(url, headers=headers) as resp:
                resp.raise_for_status()
                return await resp.text()
        except asyncio.TimeoutError:
            print(f"[TIMEOUT] ({attempt}/{FETCH_RETRIES}) {url}")
        except aiohttp.ClientResponseError as e:
            if 500 <= e.status < 600 and attempt < FETCH_RETRIES:
                print(f"[HTTP {e.status}] ì¬ì‹œë„ ({attempt}/{FETCH_RETRIES}) {url}")
            else:
                print(f"[HTTP ì˜¤ë¥˜] {url}: {e.status} {e.message}")
                return None
        except aiohttp.ClientError as e:
            print(f"[HTTP ì˜¤ë¥˜] {url}: {e}")
        if attempt < FETCH_RETRIES:
            await asyncio.sleep(RETRY_BASE_DELAY * attempt)
    return None

# --- ë§í¬ ì¶”ì¶œ (CSS ì˜ì¡´ ì œê±°, ì •ê·œì‹ ê¸°ë°˜) ---
def extract_links(html: str, base: str) -> List[str]:
    soup = BeautifulSoup(html, "lxml")
    links: List[str] = []
    for a in soup.find_all("a", href=True):
        href = a["href"].strip()
        # ìƒëŒ€ê²½ë¡œ/ìŠ¤í‚´ìƒëµ ì²˜ë¦¬
        if href.startswith("//"):
            href = "https:" + href
        elif href.startswith("/"):
            href = urljoin(base, href)
        # ìœ íš¨í•œ ê¸°ì‚¬ ë§í¬ë§Œ
        if ARTICLE_URL_RE.match(href):
            links.append(href)
    # ì¤‘ë³µ ì œê±°(ìˆœì„œ ìœ ì§€)
    seen = set()
    uniq: List[str] = []
    for u in links:
        if u not in seen:
            seen.add(u)
            uniq.append(u)
    return uniq

# --- ë³¸ë¬¸ íŒŒì‹± ---
def parse_article_details(html: str, url: str) -> Optional[Dict[str, Any]]:
    soup = BeautifulSoup(html, "lxml")

    title_tag = soup.select_one("meta[property='og:title']")
    content_tag = soup.select_one("#dic_area, #newsct_article")
    published_time_tag = soup.select_one("._ARTICLE_DATE_TIME, .media_end_head_info_datestamp_time")

    if not all([title_tag, content_tag, published_time_tag]):
        return None

    title = (title_tag.get("content") or "").strip()
    content = content_tag.get_text(separator="\n", strip=True)
    published_at = published_time_tag.get("data-date-time") or published_time_tag.get_text(strip=True)

    publisher_tag = soup.select_one(".media_end_head_top_logo_img")
    publisher = (publisher_tag.get("title") or "ì–¸ë¡ ì‚¬ ì—†ìŒ").strip() if publisher_tag else "ì–¸ë¡ ì‚¬ ì—†ìŒ"

    reporter_tag = soup.select_one(".byline_s, .media_end_head_byline")
    reporter = reporter_tag.get_text(strip=True) if reporter_tag else "ê¸°ì ì •ë³´ ì—†ìŒ"

    og_img_tag = soup.select_one("meta[property='og:image']")
    thumbnail_url = og_img_tag.get("content") if og_img_tag else None

    category_tag = soup.select_one(".media_end_categorize_item")
    category = category_tag.get_text(strip=True) if category_tag else "ê¸°íƒ€"

    return {
        "title": title,
        "content": content,
        "publisher": publisher,
        "reporter": reporter,
        "published_at": published_at,
        "url": url,
        "category": category,
        "thumbnail_url": thumbnail_url,
        "crawled_at": datetime.datetime.now(),
    }

# --- í¬ë¡¤ë§ ---
async def crawl_and_parse() -> List[Dict[str, Any]]:
    # í™ˆ + ì„¹ì…˜(ì •ì¹˜100, ê²½ì œ101, ì‚¬íšŒ102, ìƒí™œ/ë¬¸í™”103, ì„¸ê³„104, IT/ê³¼í•™105)
    seed_urls = [
        "https://news.naver.com/",
        "https://news.naver.com/section/100",
        "https://news.naver.com/section/101",
        "https://news.naver.com/section/102",
        "https://news.naver.com/section/103",
        "https://news.naver.com/section/104",
        "https://news.naver.com/section/105",
    ]

    connector = aiohttp.TCPConnector(limit=CONCURRENCY_LIMIT)
    async with aiohttp.ClientSession(timeout=REQUEST_TIMEOUT, connector=connector) as session:
        # ì‹œë“œ í˜ì´ì§€ ë™ì‹œ ìš”ì²­
        seeds = await asyncio.gather(*[fetch(session, u) for u in seed_urls])
        all_links: List[str] = []
        for html, base in zip(seeds, seed_urls):
            if html:
                all_links.extend(extract_links(html, base))

        # ì¤‘ë³µ ì œê±°
        seen = set()
        unique_links: List[str] = []
        for href in all_links:
            if href not in seen:
                seen.add(href)
                unique_links.append(href)

        if not unique_links:
            print("âŒ[ì˜¤ë¥˜] ê¸°ì‚¬ ë§í¬ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì •ê·œì‹/ì‹œë“œ URLì„ í™•ì¸í•˜ì„¸ìš”.")
            return []

        print(f"ğŸ“° ì´ {len(unique_links)}ê°œì˜ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘. ë³¸ë¬¸ ìš”ì²­ ì‹œì‘.")

        pages_html = await asyncio.gather(*[fetch(session, link) for link in unique_links])

        results: List[Dict[str, Any]] = []
        for html, url in zip(pages_html, unique_links):
            if html:
                article_data = parse_article_details(html, url)
                if article_data:
                    results.append(article_data)

        return results

# --- DB ì €ì¥ ---
def save_to_mysql(headlines: List[Dict[str, Any]]):
    if not headlines:
        print("ğŸ“­ ì €ì¥í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    insert_data = [
        (
            d["title"],
            d["content"],
            d["publisher"],
            d["reporter"],
            d["published_at"],
            d["url"],
            d["category"],
            d["thumbnail_url"],
            d["crawled_at"],
        )
        for d in headlines
    ]

    query = """
        INSERT INTO news (
            title, content, publisher, reporter, published_at,
            url, category, thumbnail_url, crawled_at
        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
        ON DUPLICATE KEY UPDATE url = VALUES(url)
    """

    try:
        with mysql.connector.connect(**DB_CONFIG) as conn:
            with conn.cursor() as cursor:
                cursor.executemany(query, insert_data)
                conn.commit()
                print(f"âœ… DB ì²˜ë¦¬ ì™„ë£Œ (ìš”ì²­ ê±´ìˆ˜: {len(insert_data)})")
    except mysql.connector.Error as e:
        print(f"[DB ì˜¤ë¥˜] ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")

# --- ì‹¤í–‰ ---
if __name__ == "__main__":
    print(f"ğŸš€ {datetime.datetime.now()} - ë‰´ìŠ¤ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    news_articles = asyncio.run(crawl_and_parse())

    if news_articles:
        print(f"âœ”ï¸ ì´ {len(news_articles)}ê±´ì˜ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        save_to_mysql(news_articles)
    else:
        print("ğŸ”´ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

    print("âœ¨ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
