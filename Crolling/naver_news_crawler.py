import requests
from bs4 import BeautifulSoup
import mysql.connector
import datetime

def crawl_naver_news():
    url = "https://news.naver.com/"
    headers = {
        "User-Agent": "Mozilla/5.0"
    }

    try:
        response = requests.get(url, headers=headers, timeout=5)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"[ERROR] ìš”ì²­ ì‹¤íŒ¨: {e}")
        return []

    soup = BeautifulSoup(response.text, "html.parser")
    headlines = []

    title_tags = soup.select("a[href^='https://n.news.naver.com']")

    for tag in title_tags:
        title = tag.get_text(strip=True)
        link = tag.get("href")
        if not link.startswith("http"):
            link = "https://news.naver.com" + link

        try:
            detail = requests.get(link, headers=headers, timeout=5)
            detail.raise_for_status()
            detail_soup = BeautifulSoup(detail.text, "html.parser")

            content = detail_soup.select_one("#dic_area")
            publisher_img = detail_soup.select_one(".press_logo img")
            publisher_meta = detail_soup.select_one("meta[property='og:article:author']")
            reporter = detail_soup.select_one(".byline_s")
            published_time_tag = detail_soup.select_one("._ARTICLE_DATE_TIME[data-date-time]")
            thumbnail = detail_soup.select_one("meta[property='og:image']")
            category = "ê¸°íƒ€"

            published_at = published_time_tag.get("data-date-time") if published_time_tag else None
            if not published_at:
                print(f"[ìŠ¤í‚µ] ë°œí–‰ ì‹œê°„ ì—†ìŒ: {link}")
                continue

            # ê¸°ìê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
            if not reporter:
                print(f"[ìŠ¤í‚µ] ê¸°ì ì—†ìŒ: {title}")
                continue

            publisher = publisher_img.get("alt") if publisher_img else (
                publisher_meta.get("content") if publisher_meta else "ì•Œ ìˆ˜ ì—†ìŒ"
            )

            headlines.append({
                "title": title,
                "content": content.get_text(strip=True) if content else "",
                "publisher": publisher,
                "reporter": reporter.get_text(strip=True),
                "published_at": published_at,
                "url": link,
                "category": category,
                "thumbnail_url": thumbnail.get("content") if thumbnail else None,
                "crawled_at": datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            })

        except requests.exceptions.Timeout:
            print(f"[TIMEOUT] íƒ€ì„ì•„ì›ƒ ë°œìƒ: {link}")
            continue
        except Exception as e:
            print(f"[WARN] ìƒì„¸ í¬ë¡¤ë§ ì‹¤íŒ¨ ({link}): {e}")
            continue

    return headlines

def save_to_mysql(headlines):
    try:
        conn = mysql.connector.connect(
            host="localhost",
            user="root",
            password="1234",  # ë¹„ë°€ë²ˆí˜¸ ë§ê²Œ ìˆ˜ì •
            database="risetest",
            port=3306
        )
        cursor = conn.cursor()
        inserted = 0

        for item in headlines:
            try:
                cursor.execute("""
                    INSERT INTO news (
                        title, content, publisher, reporter, published_at,
                        url, category, thumbnail_url, crawled_at
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                """, (
                    item["title"],
                    item["content"],
                    item["publisher"],
                    item["reporter"],
                    item["published_at"],
                    item["url"],
                    item["category"],
                    item["thumbnail_url"],
                    item["crawled_at"]
                ))
                inserted += 1
            except mysql.connector.IntegrityError:
                print(f"[ì¤‘ë³µ] ìŠ¤í‚µ: {item['published_at']} {item['title']}")
                continue

        conn.commit()
        print(f"âœ… ì´ {inserted}ê±´ ì €ì¥ ì™„ë£Œ.")
    except mysql.connector.Error as e:
        print(f"[DB ì˜¤ë¥˜] {e}")
    finally:
        if 'cursor' in locals():
            cursor.close()
        if 'conn' in locals() and conn.is_connected():
            conn.close()

if __name__ == "__main__":
    print("ğŸ•’ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘...")
    data = crawl_naver_news()
    if data:
        save_to_mysql(data)
    else:
        print("ğŸ“­ ë‰´ìŠ¤ ì—†ìŒ.")
